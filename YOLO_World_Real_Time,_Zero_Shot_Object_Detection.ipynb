{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSqqBt2LpKiZQcLOl/qBqv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BergerPerkins/Computer-Vision/blob/main/YOLO_World_Real_Time%2C_Zero_Shot_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ultralytics"
      ],
      "metadata": {
        "id": "QtK7L_VVv1QU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics\n",
        "ultralytics.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "K5zLdM9vv1L_",
        "outputId": "a6dadc9c-be4b-443e-c2e0-076d815a21b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8.1.18'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **loading a pre-trained yolo-world model and running a prediction on an image.**"
      ],
      "metadata": {
        "id": "w1X7zPwowOfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLOWorld\n",
        "\n",
        "# Initialize a YOLO-World model\n",
        "model = YOLOWorld('yolov8s-world.pt')  # or select yolov8m/l-world.pt for different sizes\n",
        "\n",
        "# Execute inference with the YOLOv8s-world model on the specified image\n",
        "results = model.predict('/content/1.png', device='cpu', save=True)\n",
        "\n",
        "# Show results\n",
        "results[0].show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeoyG4FMwKVR",
        "outputId": "243878ca-3662-4b21-d28d-e799816bbddf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8s-world.pt to 'yolov8s-world.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25.9M/25.9M [00:00<00:00, 139MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ yolov8s-world.pt appears to require 'dill', which is not in ultralytics requirements.\n",
            "AutoInstall will run now for 'dill' but this feature will be removed in the future.\n",
            "Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official YOLOv8 model, i.e. 'yolo predict model=yolov8n.pt'\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['dill'] not found, attempting AutoUpdate...\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 1.8 MB/s eta 0:00:00\n",
            "Installing collected packages: dill\n",
            "Successfully installed dill-0.3.8\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 5.7s, installed 1 package: ['dill']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "image 1/1 /content/1.png: 544x640 4 persons, 1 car, 1 bus, 1 backpack, 1 handbag, 2 chairs, 647.4ms\n",
            "Speed: 14.2ms preprocess, 647.4ms inference, 16.4ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set prompts for specific tasks**\n",
        "The YOLO-World framework allows for the dynamic specification of classes through custom prompts, empowering users to tailor the model to their specific needs without retraining."
      ],
      "metadata": {
        "id": "_0KlggCdwqOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Initialize a YOLO-World model\n",
        "model = YOLO('yolov8s-world.pt')  # or choose yolov8m/l-world.pt\n",
        "\n",
        "# Define custom classes\n",
        "model.set_classes([\"person\", \"bus\"])\n",
        "\n",
        "# Execute prediction for specified categories on an image\n",
        "results = model.predict('/content/1.png', device='cpu', save=True)\n",
        "\n",
        "# Show results\n",
        "results[0].show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI-V5aQtws8g",
        "outputId": "078fc36e-9246-4ffb-83f2-db84651b967b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['git+https://github.com/openai/CLIP.git'] not found, attempting AutoUpdate...\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-prjvvnvc\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.4/53.4 kB 1.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py): started\n",
            "  Building wheel for clip (setup.py): finished with status 'done'\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=ca93eb6049c3b421c7c9406954cbef3c86fda9b9cc128e6f22b09fa7d0e77fe7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jlael8s0/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.3\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 8.1s, installed 1 package: ['git+https://github.com/openai/CLIP.git']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 159MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/1.png: 544x640 4 persons, 742.3ms\n",
            "Speed: 8.1ms preprocess, 742.3ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You can also save a model after setting custom classes. By doing this you create a version of the YOLO-World model that is specialized for your specific use case.**"
      ],
      "metadata": {
        "id": "cRd-aT1Zw9A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Initialize a YOLO-World model\n",
        "model = YOLO('yolov8s-world.pt')  # or select yolov8m/l-world.pt\n",
        "\n",
        "# Define custom classes\n",
        "model.set_classes([\"person\"])\n",
        "\n",
        "# Save the model with the defined offline vocabulary\n",
        "model.save(\"custom_yolov8s.pt\")"
      ],
      "metadata": {
        "id": "Qa_qjtnhw96o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After saving, the custom_yolov8s.pt model behaves like any other pre-trained YOLOv8 model but with a key difference: it is now optimized to detect only the classes you have defined.**\n",
        "#**Load custom model and perform predictions**"
      ],
      "metadata": {
        "id": "WzPaMoiexE-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load your custom model\n",
        "model = YOLO('/content/custom_yolov8s.pt')\n",
        "\n",
        "# Run inference to detect your custom classes\n",
        "results = model.predict('/content/2.jpg', device='cpu')\n",
        "\n",
        "# Show results\n",
        "results[0].show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYRJXbndxH3t",
        "outputId": "573036a7-0595-40d8-e82d-52f497e16c0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/2.jpg: 448x640 2 persons, 382.1ms\n",
            "Speed: 2.6ms preprocess, 382.1ms inference, 3.7ms postprocess per image at shape (1, 3, 448, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load your custom model\n",
        "model = YOLO('/content/custom_yolov8s.pt')\n",
        "\n",
        "# Run inference to detect your custom classes\n",
        "results = model.predict('/content/2.jpg')\n",
        "\n",
        "# Show results\n",
        "results[0].show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSQcwshYyK2O",
        "outputId": "20cad6c9-aed9-49d0-8caa-6f02a0d0196f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/2.jpg: 448x640 2 persons, 376.6ms\n",
            "Speed: 3.2ms preprocess, 376.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0F-8No7zN0-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}